{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "improved-nothing",
   "metadata": {},
   "source": [
    "# Problem Definition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "encouraging-comedy",
   "metadata": {},
   "source": [
    "The current project is a Text Classification problem, specifically Sentiment Analysis. The dataset we will be using is the Large Movie Review Dataset, which can be found at http://ai.stanford.edu/~amaas/data/sentiment/. We will try to solve the sentiment analysis problem to classify movie reviews as either positive or negative. This will be a supervised text classification problem because both the training data and test data movie reviews contain labels for either positive sentiment or negative sentiment. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sporting-envelope",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "classical-connection",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "civil-operator",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-11T20:54:07.640240Z",
     "start_time": "2021-04-11T20:54:07.629270Z"
    }
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "dynamic-lexington",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-11T20:54:51.358766Z",
     "start_time": "2021-04-11T20:54:11.562337Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file</th>\n",
       "      <th>text</th>\n",
       "      <th>is_positive</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0_9.txt</td>\n",
       "      <td>b'Bromwell High is a cartoon comedy. It ran at...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10000_8.txt</td>\n",
       "      <td>b'Homelessness (or Houselessness as George Car...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10001_10.txt</td>\n",
       "      <td>b'Brilliant over-acting by Lesley Ann Warren. ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10002_7.txt</td>\n",
       "      <td>b'This is easily the most underrated film inn ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10003_8.txt</td>\n",
       "      <td>b'This is not the typical Mel Brooks film. It ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24995</th>\n",
       "      <td>9998_4.txt</td>\n",
       "      <td>b\"Towards the end of the movie, I felt it was ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24996</th>\n",
       "      <td>9999_3.txt</td>\n",
       "      <td>b'This is the kind of movie that my enemies co...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24997</th>\n",
       "      <td>999_3.txt</td>\n",
       "      <td>b\"I saw 'Descent' last night at the Stockholm ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24998</th>\n",
       "      <td>99_1.txt</td>\n",
       "      <td>b\"Some films that you pick up for a pound turn...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24999</th>\n",
       "      <td>9_1.txt</td>\n",
       "      <td>b\"This is one of the dumbest films, I've ever ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>25000 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               file                                               text  \\\n",
       "0           0_9.txt  b'Bromwell High is a cartoon comedy. It ran at...   \n",
       "1       10000_8.txt  b'Homelessness (or Houselessness as George Car...   \n",
       "2      10001_10.txt  b'Brilliant over-acting by Lesley Ann Warren. ...   \n",
       "3       10002_7.txt  b'This is easily the most underrated film inn ...   \n",
       "4       10003_8.txt  b'This is not the typical Mel Brooks film. It ...   \n",
       "...             ...                                                ...   \n",
       "24995    9998_4.txt  b\"Towards the end of the movie, I felt it was ...   \n",
       "24996    9999_3.txt  b'This is the kind of movie that my enemies co...   \n",
       "24997     999_3.txt  b\"I saw 'Descent' last night at the Stockholm ...   \n",
       "24998      99_1.txt  b\"Some films that you pick up for a pound turn...   \n",
       "24999       9_1.txt  b\"This is one of the dumbest films, I've ever ...   \n",
       "\n",
       "       is_positive  \n",
       "0                1  \n",
       "1                1  \n",
       "2                1  \n",
       "3                1  \n",
       "4                1  \n",
       "...            ...  \n",
       "24995            0  \n",
       "24996            0  \n",
       "24997            0  \n",
       "24998            0  \n",
       "24999            0  \n",
       "\n",
       "[25000 rows x 3 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_data(split):\n",
    "    if split.lower() == \"train\":\n",
    "        folder = \"train\"\n",
    "    elif split.lower() == \"test\":\n",
    "        folder = \"test\"\n",
    "    else:\n",
    "        raise ValueError(\"Invalid data split specified.\")\n",
    "\n",
    "    file_names = []\n",
    "    text = []\n",
    "    is_positive = []\n",
    "\n",
    "    # read all positive files\n",
    "    files = glob.glob(os.path.join(\"data\", folder, \"pos\", \"*\"))\n",
    "    for file in files:\n",
    "        head, tail = os.path.split(file)\n",
    "        file_names.append(tail)\n",
    "        is_positive.append(1)\n",
    "        with open(file, \"rb\") as open_file:\n",
    "            text.append(open_file.readlines()[0])\n",
    "\n",
    "    # read all negative files\n",
    "    files = glob.glob(os.path.join(\"data\", folder, \"neg\", \"*\"))\n",
    "    for file in files:\n",
    "        head, tail = os.path.split(file)\n",
    "        file_names.append(tail)\n",
    "        is_positive.append(0)\n",
    "        with open(file, \"rb\") as open_file:\n",
    "            text.append(open_file.readlines()[0])\n",
    "\n",
    "    return pd.DataFrame(\n",
    "        data={\"file\": file_names, \"text\": text, \"is_positive\": is_positive}\n",
    "    )\n",
    "\n",
    "\n",
    "train_df = get_data(\"train\")\n",
    "train_df[\"text\"] = train_df[\"text\"].astype(str)\n",
    "train_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "seeing-genius",
   "metadata": {},
   "source": [
    "### Get only 5000 records from the original 25000 highly polar movie reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "mexican-parking",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-11T20:55:32.979322Z",
     "start_time": "2021-04-11T20:55:32.962338Z"
    }
   },
   "outputs": [],
   "source": [
    "positive_reviews = train_df[:2500]  # take first 2500 reviews which are positive\n",
    "negative_reviews = train_df[22500:]  # last last 2500 reviews which are negative\n",
    "new_train = positive_reviews.append(negative_reviews, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sacred-mobility",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-11T19:05:25.075085Z",
     "start_time": "2021-04-11T19:05:25.049641Z"
    }
   },
   "outputs": [],
   "source": [
    "new_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "potential-compression",
   "metadata": {},
   "source": [
    "### Basic Properties of the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "enabling-thanksgiving",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list column names and datatypes\n",
    "new_train.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ethical-daisy",
   "metadata": {},
   "outputs": [],
   "source": [
    "# length of dataframe\n",
    "len(new_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "automotive-harmony",
   "metadata": {},
   "source": [
    "######  Tokenization is the process of breaking down chunks of text into smaller pieces.  Word tokenization separates text into individual words.\n",
    "###### We will then filter out stop words to remove common English words such as \"the\", \"and\", \"him\" which will not be needed for modelling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fancy-hughes",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-11T19:24:58.401346Z",
     "start_time": "2021-04-11T19:23:49.828450Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\yunan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file</th>\n",
       "      <th>text</th>\n",
       "      <th>is_positive</th>\n",
       "      <th>docs</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0_9.txt</td>\n",
       "      <td>b'Bromwell High is a cartoon comedy. It ran at...</td>\n",
       "      <td>1</td>\n",
       "      <td>bromwell high cartoon comedy ran time program ...</td>\n",
       "      <td>[(bromwel, RB), (high, JJ), (cartoon, NN), (co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10000_8.txt</td>\n",
       "      <td>b'Homelessness (or Houselessness as George Car...</td>\n",
       "      <td>1</td>\n",
       "      <td>homelessness houselessness george carlin state...</td>\n",
       "      <td>[(homeless, NN), (houseless, NN), (georg, NN),...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10001_10.txt</td>\n",
       "      <td>b'Brilliant over-acting by Lesley Ann Warren. ...</td>\n",
       "      <td>1</td>\n",
       "      <td>brilliant acting lesley ann warren best dramat...</td>\n",
       "      <td>[(brilliant, JJ), (over-act, JJ), (lesley, NN)...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10002_7.txt</td>\n",
       "      <td>b'This is easily the most underrated film inn ...</td>\n",
       "      <td>1</td>\n",
       "      <td>easily underrated film inn brook cannon sure f...</td>\n",
       "      <td>[(easili, NN), (underr, JJ), (film, NN), (inn,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10003_8.txt</td>\n",
       "      <td>b'This is not the typical Mel Brooks film. It ...</td>\n",
       "      <td>1</td>\n",
       "      <td>typical mel brook film slapstick movie actuall...</td>\n",
       "      <td>[(typic, NN), (mel, NN), (brook, NN), (film, N...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4995</th>\n",
       "      <td>9998_4.txt</td>\n",
       "      <td>b\"Towards the end of the movie, I felt it was ...</td>\n",
       "      <td>0</td>\n",
       "      <td>end movie felt technical felt like classroom w...</td>\n",
       "      <td>[(end, NN), (movi, NN), (felt, VBD), (technic,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4996</th>\n",
       "      <td>9999_3.txt</td>\n",
       "      <td>b'This is the kind of movie that my enemies co...</td>\n",
       "      <td>0</td>\n",
       "      <td>kind movie enemy content watch time bloody tru...</td>\n",
       "      <td>[(kind, NN), (movi, NN), (enemi, NN), (content...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4997</th>\n",
       "      <td>999_3.txt</td>\n",
       "      <td>b\"I saw 'Descent' last night at the Stockholm ...</td>\n",
       "      <td>0</td>\n",
       "      <td>saw descent night stockholm film festival huge...</td>\n",
       "      <td>[(saw, JJ), (descent, NN), (night, NN), (stock...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4998</th>\n",
       "      <td>99_1.txt</td>\n",
       "      <td>b\"Some films that you pick up for a pound turn...</td>\n",
       "      <td>0</td>\n",
       "      <td>film pick pound turn good 23rd century film re...</td>\n",
       "      <td>[(film, NN), (pick, NN), (pound, NN), (turn, V...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4999</th>\n",
       "      <td>9_1.txt</td>\n",
       "      <td>b\"This is one of the dumbest films, I've ever ...</td>\n",
       "      <td>0</td>\n",
       "      <td>dumbest film seen rip nearly type thriller man...</td>\n",
       "      <td>[(dumbest, NN), (film, NN), (seen, VBN), (rip,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5000 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              file                                               text  \\\n",
       "0          0_9.txt  b'Bromwell High is a cartoon comedy. It ran at...   \n",
       "1      10000_8.txt  b'Homelessness (or Houselessness as George Car...   \n",
       "2     10001_10.txt  b'Brilliant over-acting by Lesley Ann Warren. ...   \n",
       "3      10002_7.txt  b'This is easily the most underrated film inn ...   \n",
       "4      10003_8.txt  b'This is not the typical Mel Brooks film. It ...   \n",
       "...            ...                                                ...   \n",
       "4995    9998_4.txt  b\"Towards the end of the movie, I felt it was ...   \n",
       "4996    9999_3.txt  b'This is the kind of movie that my enemies co...   \n",
       "4997     999_3.txt  b\"I saw 'Descent' last night at the Stockholm ...   \n",
       "4998      99_1.txt  b\"Some films that you pick up for a pound turn...   \n",
       "4999       9_1.txt  b\"This is one of the dumbest films, I've ever ...   \n",
       "\n",
       "      is_positive                                               docs  \\\n",
       "0               1  bromwell high cartoon comedy ran time program ...   \n",
       "1               1  homelessness houselessness george carlin state...   \n",
       "2               1  brilliant acting lesley ann warren best dramat...   \n",
       "3               1  easily underrated film inn brook cannon sure f...   \n",
       "4               1  typical mel brook film slapstick movie actuall...   \n",
       "...           ...                                                ...   \n",
       "4995            0  end movie felt technical felt like classroom w...   \n",
       "4996            0  kind movie enemy content watch time bloody tru...   \n",
       "4997            0  saw descent night stockholm film festival huge...   \n",
       "4998            0  film pick pound turn good 23rd century film re...   \n",
       "4999            0  dumbest film seen rip nearly type thriller man...   \n",
       "\n",
       "                                                 tokens  \n",
       "0     [(bromwel, RB), (high, JJ), (cartoon, NN), (co...  \n",
       "1     [(homeless, NN), (houseless, NN), (georg, NN),...  \n",
       "2     [(brilliant, JJ), (over-act, JJ), (lesley, NN)...  \n",
       "3     [(easili, NN), (underr, JJ), (film, NN), (inn,...  \n",
       "4     [(typic, NN), (mel, NN), (brook, NN), (film, N...  \n",
       "...                                                 ...  \n",
       "4995  [(end, NN), (movi, NN), (felt, VBD), (technic,...  \n",
       "4996  [(kind, NN), (movi, NN), (enemi, NN), (content...  \n",
       "4997  [(saw, JJ), (descent, NN), (night, NN), (stock...  \n",
       "4998  [(film, NN), (pick, NN), (pound, NN), (turn, V...  \n",
       "4999  [(dumbest, NN), (film, NN), (seen, VBN), (rip,...  \n",
       "\n",
       "[5000 rows x 5 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import movie_reviews  #  this is the corpus we will use\n",
    "from nltk.stem.snowball import SnowballStemmer  #  Snowball stemmer > Porter stemmer\n",
    "from nltk.tokenize.casual import (\n",
    "    casual_tokenize,\n",
    ")  # we use casual tokenize because this is colloquial text\n",
    "from sklearn.feature_extraction.text import (\n",
    "    ENGLISH_STOP_WORDS as sklearn_stop_words,\n",
    ")  # sklearn stop words is larger than nltk\n",
    "\n",
    "import re\n",
    "\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "nltk.download(\"wordnet\")\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from collections import defaultdict\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "\n",
    "def decontracted(phrase):\n",
    "    # Taken from https://stackoverflow.com/questions/19790188/expanding-english-language-contractions-in-python\n",
    "\n",
    "    # specific\n",
    "    phrase = re.sub(r\"won\\'t\", \"will not\", phrase)\n",
    "    phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n",
    "\n",
    "    # general\n",
    "    phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n",
    "    phrase = re.sub(r\"\\'s\", \" is\", phrase)\n",
    "    phrase = re.sub(r\"\\'d\", \" would\", phrase)\n",
    "    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n",
    "    phrase = re.sub(r\"\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n",
    "    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n",
    "    return phrase\n",
    "\n",
    "\n",
    "def tokenize_phrase(text):\n",
    "    phrase = decontracted(\n",
    "        text.replace(\"\\\\\", \"\").replace(\"b'\", \"\")\n",
    "    )  # remove backslashes and replace contractions\n",
    "    tokens = casual_tokenize(phrase, reduce_len=True, strip_handles=True)\n",
    "    normalized_tokens = [x.lower() for x in tokens]  #  convert to all lowercase\n",
    "    filtered_tokens = [\n",
    "        x for x in normalized_tokens if x not in sklearn_stop_words\n",
    "    ]  #  filter stop words\n",
    "    filtered_tokens = [\n",
    "        x\n",
    "        for x in filtered_tokens\n",
    "        if x and x not in \"- \\t\\n.\\\"':[...][\\\\]()/[br]<>*~,;!?\"\n",
    "    ]  #  filter punctuations\n",
    "    lemmatized_tokes = [\n",
    "        lemmatizer.lemmatize(t) for t in tokens\n",
    "    ]  # perform lemmatization\n",
    "    stemmed_tokens = [stemmer.stem(w) for w in filtered_tokens]  # perform stemming\n",
    "    pos_tokens = nltk.pos_tag(stemmed_tokens)  # perform part-of-speech tagging\n",
    "    return pos_tokens\n",
    "\n",
    "\n",
    "new_train[\"tokens\"] = new_train[\"text\"].apply(tokenize_phrase)\n",
    "new_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "presidential-andrew",
   "metadata": {},
   "source": [
    "Convert tokens to their own DataFrame so we can perform descriptive statistics on them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "experimental-renewal",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-11T19:25:12.926864Z",
     "start_time": "2021-04-11T19:25:10.427518Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token</th>\n",
       "      <th>part_of_speech</th>\n",
       "      <th>is_positive</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>bromwel</td>\n",
       "      <td>RB</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>high</td>\n",
       "      <td>JJ</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>cartoon</td>\n",
       "      <td>NN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>comedi</td>\n",
       "      <td>NN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ran</td>\n",
       "      <td>VBD</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>538085</th>\n",
       "      <td>time</td>\n",
       "      <td>NN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>538086</th>\n",
       "      <td>money</td>\n",
       "      <td>NN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>538087</th>\n",
       "      <td>wast</td>\n",
       "      <td>NN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>538088</th>\n",
       "      <td>time</td>\n",
       "      <td>NN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>538089</th>\n",
       "      <td>pain</td>\n",
       "      <td>NN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>538090 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          token part_of_speech  is_positive\n",
       "0       bromwel             RB            1\n",
       "1          high             JJ            1\n",
       "2       cartoon             NN            1\n",
       "3        comedi             NN            1\n",
       "4           ran            VBD            1\n",
       "...         ...            ...          ...\n",
       "538085     time             NN            0\n",
       "538086    money             NN            0\n",
       "538087     wast             NN            0\n",
       "538088     time             NN            0\n",
       "538089     pain             NN            0\n",
       "\n",
       "[538090 rows x 3 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import itertools\n",
    "\n",
    "new_train[\"tokens_with_positive\"] = new_train.apply(\n",
    "    lambda x: [token + (x[\"is_positive\"],) for token in x[\"tokens\"]], axis=1\n",
    ")\n",
    "token_df = pd.DataFrame(\n",
    "    list(itertools.chain.from_iterable(new_train[\"tokens_with_positive\"])),\n",
    "    columns=[\"token\", \"part_of_speech\", \"is_positive\"],\n",
    ")\n",
    "token_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pending-warehouse",
   "metadata": {},
   "source": [
    "# Data Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "industrial-moldova",
   "metadata": {},
   "source": [
    "Some descriptive statistics about the tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "diverse-timber",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_df[[\"token\", \"is_positive\"]].groupby(\"token\").count().rename(\n",
    "    columns={\"is_positive\": \"count\"}\n",
    ").sort_values(by=[\"count\"], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "coupled-audit",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_df[[\"part_of_speech\", \"is_positive\"]].groupby(\"part_of_speech\").count().rename(\n",
    "    columns={\"is_positive\": \"count\"}\n",
    ").sort_values(by=[\"count\"], ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "friendly-manufacturer",
   "metadata": {},
   "source": [
    "Normalization of words is condensing different forms of the same word into a single word. For example, \"watched\", \"watching\" and \"watches\" will be normalized to \"watch\".\n",
    "\n",
    "Two main ways for normalization is through stemmming and lemmatization. In stemming, the common word endings such as \"-ing\", \"s\", \"-ed\", \"-de\" are removed from the smallest unit of the word. There may be some issues with words which have special conjugations, for example \"feel\" and \"felt\", since stemming just removes common word endings from the string. The relationship betwen \"feel\" and \"felt\" may not be recognized, and the stemming process may result in words which are not actual words.\n",
    "\n",
    "Lemmatization uses a data structure to relate different forms of a word to its dictionary form or root word (lemma). Lemmatization will return an actual word in a language.\n",
    "\n",
    "Modified from https://realpython.com/sentiment-analysis-python/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "improved-barrier",
   "metadata": {},
   "source": [
    "Descriptive statistics of tokens and part-of-speech tagging, indicating there are 29,531 unique words and 38 unique pos in our sample reviews, and the most frequent word is \"movie\" and pos is \"NN\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pleased-vault",
   "metadata": {},
   "outputs": [],
   "source": [
    "# describe categorical columns of token_df - unique, top and frequency of token and part-of-speech\n",
    "token_df.describe(include=np.object).transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "widespread-finnish",
   "metadata": {},
   "source": [
    "Look at the number of words for positive and negative reviews. It shows positive reviews generally have (275637-262453)/2500=5.27 more words than negative ones per review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "higher-accreditation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# group by positivity, count distinct pos and number of words\n",
    "posneg_df = (\n",
    "    token_df.groupby(\"is_positive\")\n",
    "    .agg({\"part_of_speech\": pd.Series.nunique, \"token\": pd.Series.count})\n",
    "    .rename(columns={\"pos\": \"pos_unique\", \"token\": \"token_count\"})\n",
    "    .sort_values(\"token_count\", ascending=False)\n",
    ")\n",
    "\n",
    "# show top 5 records\n",
    "posneg_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "engaged-madrid",
   "metadata": {},
   "source": [
    "Look at the positive review rates, as well as the number of words, by each pos. It's shown that reviews contain NN (common nouns) the most, followed by JJ (adjectives). 59.4% PRP and 58.4% RBS are from positive reviews, which are the highest among all pos, indicating when reviewers use more PRP (personal pronouns) and RBS (adverb superlative), they tend to give positive reviews, say, than using WP (Wh-pronouns such as what, which, who, whoever)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "talented-inspiration",
   "metadata": {},
   "outputs": [],
   "source": [
    "# group by part-of-speech, count positivity rates and number of words by POS\n",
    "pos_df = (\n",
    "    token_df.groupby(\"part_of_speech\")\n",
    "    .agg({\"is_positive\": pd.Series.mean, \"token\": pd.Series.count})\n",
    "    .rename(columns={\"is_positive\": \"pos_rate\", \"token\": \"token_count\"})\n",
    "    .sort_values(\"token_count\", ascending=False)\n",
    ")\n",
    "\n",
    "# show top 10 records\n",
    "pos_df.head"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "secret-moment",
   "metadata": {},
   "source": [
    "Look at the frequency of each token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "complicated-manual",
   "metadata": {},
   "outputs": [],
   "source": [
    "# group by token, count frequency of each token\n",
    "numtoken_df = (\n",
    "    token_df.groupby(\"token\")\n",
    "    .agg({\"token\": pd.Series.count})\n",
    "    .rename(columns={\"token\": \"token_count\"})\n",
    "    .sort_values(\"token_count\", ascending=False)\n",
    ")\n",
    "\n",
    "# show top 10 records\n",
    "numtoken_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "qualified-affiliate",
   "metadata": {},
   "source": [
    "Five number summary for the number of tokens dataframe. The token counts by part of speech range from just 1  to 10057 counts for \"movie\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "limited-frame",
   "metadata": {},
   "outputs": [],
   "source": [
    "numtoken_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "appreciated-chair",
   "metadata": {},
   "source": [
    "## Data Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "swiss-thing",
   "metadata": {},
   "outputs": [],
   "source": [
    "# horizontal boxplot of a dataframe column - frequency of tokens\n",
    "numtoken_df[[\"token_count\"]].plot(kind=\"box\", vert=False, figsize=(13, 5));"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "norwegian-triumph",
   "metadata": {},
   "source": [
    "Frequency distribution of each token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "removable-spice",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bar chart of a dataframe column - print the frequency of top 60 tokens\n",
    "numtoken_df[[\"token_count\"]][:60].plot(kind=\"bar\", figsize=(12, 7));"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "irish-adult",
   "metadata": {},
   "source": [
    "Frequency distribution of tokens after removing stopwords again from a differnt dictionary (SpaCy)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hawaiian-richmond",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "from collections import Counter\n",
    "\n",
    "# remove non-meaningful SpaCy stopwords again\n",
    "def remove_stopwords(tokens):\n",
    "    \"\"\"Remove stopwords from a list of tokens.\"\"\"\n",
    "    return [t for t in tokens if t not in STOP_WORDS]\n",
    "\n",
    "\n",
    "counter = Counter(remove_stopwords(token_df[\"token\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "incorporate-redhead",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert list of tuples into data frame - plot the top 40 tokens\n",
    "freq_df = pd.DataFrame.from_records(counter.most_common(40), columns=[\"token\", \"count\"])\n",
    "\n",
    "# create bar plot\n",
    "freq_df.plot(kind=\"bar\", x=\"token\", figsize=(12, 7));"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "current-oriental",
   "metadata": {},
   "source": [
    "WordCloud for all reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "experimental-titanium",
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "\n",
    "\n",
    "def wordcloud(counter):\n",
    "    \"\"\"A small wordloud wrapper\"\"\"\n",
    "    wc = WordCloud(width=1200, height=800, background_color=\"white\", max_words=200)\n",
    "    wc.generate_from_frequencies(counter)\n",
    "\n",
    "    # Plot\n",
    "    fig = plt.figure(figsize=(6, 4))\n",
    "    plt.imshow(wc, interpolation=\"bilinear\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.tight_layout(pad=0)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abandoned-intermediate",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create wordcloud\n",
    "wordcloud(counter)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beginning-asian",
   "metadata": {},
   "source": [
    "WordCloud for positive reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "expected-camcorder",
   "metadata": {},
   "outputs": [],
   "source": [
    "# wordcloud for positive reviews\n",
    "counter2 = Counter(remove_stopwords(token_df[token_df[\"is_positive\"] == 1][\"token\"]))\n",
    "wordcloud(counter2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "registered-holiday",
   "metadata": {},
   "source": [
    "WordCloud for negative reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "disturbed-therapy",
   "metadata": {},
   "outputs": [],
   "source": [
    "# wordcloud for negative reviews\n",
    "counter3 = Counter(remove_stopwords(token_df[token_df[\"is_positive\"] == 0][\"token\"]))\n",
    "wordcloud(counter3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "quiet-hospital",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cross-beaver",
   "metadata": {},
   "source": [
    "## Bag of Words features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "theoretical-birmingham",
   "metadata": {},
   "source": [
    "Attempt to use VADER to construct sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "floating-anger",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "sa = SentimentIntensityAnalyzer()\n",
    "# we not use lemmatized strings as vader considers tense of the word\n",
    "new_train['vader_compound'] = new_train['text'].apply(lambda text: sa.polarity_scores(text.replace(\"\\\\\", \"\").replace(\"b'\", \"\"))['compound'])\n",
    "# -1 to -0.05 = negative\n",
    "# -0.05 to 0.05 = neutral\n",
    "# 0.05 to 1 = positive\n",
    "new_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fitted-james",
   "metadata": {},
   "source": [
    "Attempt to use Naive Bayes to try and construct sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "electronic-executive",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-11T19:26:38.790321Z",
     "start_time": "2021-04-11T19:25:17.014694Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>(bromwel, RB)</th>\n",
       "      <th>(high, JJ)</th>\n",
       "      <th>(cartoon, NN)</th>\n",
       "      <th>(comedi, NN)</th>\n",
       "      <th>(ran, VBD)</th>\n",
       "      <th>(time, NN)</th>\n",
       "      <th>(program, NN)</th>\n",
       "      <th>(school, NN)</th>\n",
       "      <th>(life, NN)</th>\n",
       "      <th>(teacher, RB)</th>\n",
       "      <th>...</th>\n",
       "      <th>(inferno, JJ)</th>\n",
       "      <th>(lol, VBP)</th>\n",
       "      <th>(american-canadian, JJ)</th>\n",
       "      <th>(hardgor, JJ)</th>\n",
       "      <th>(chronicl, VBP)</th>\n",
       "      <th>(lego, NN)</th>\n",
       "      <th>(whelk, JJ)</th>\n",
       "      <th>(18,000, CD)</th>\n",
       "      <th>(ill, NNS)</th>\n",
       "      <th>(shite, JJ)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4995</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4996</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4997</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4998</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4999</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5000 rows Ã— 52230 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      (bromwel, RB)  (high, JJ)  (cartoon, NN)  (comedi, NN)  (ran, VBD)  \\\n",
       "0                 1           5              1             1           1   \n",
       "1                 0           0              0             0           0   \n",
       "2                 0           0              0             0           0   \n",
       "3                 0           0              0             0           0   \n",
       "4                 0           0              0             0           0   \n",
       "...             ...         ...            ...           ...         ...   \n",
       "4995              0           0              0             0           0   \n",
       "4996              0           0              0             0           0   \n",
       "4997              0           0              0             0           0   \n",
       "4998              0           0              0             0           0   \n",
       "4999              0           0              0             0           0   \n",
       "\n",
       "      (time, NN)  (program, NN)  (school, NN)  (life, NN)  (teacher, RB)  ...  \\\n",
       "0              1              1             3           1              2  ...   \n",
       "1              0              0             1           1              0  ...   \n",
       "2              1              0             0           0              0  ...   \n",
       "3              0              0             0           0              0  ...   \n",
       "4              0              0             0           0              0  ...   \n",
       "...          ...            ...           ...         ...            ...  ...   \n",
       "4995           0              0             0           0              0  ...   \n",
       "4996           1              0             0           0              0  ...   \n",
       "4997           0              0             0           0              0  ...   \n",
       "4998           1              0             0           0              0  ...   \n",
       "4999           2              0             0           0              0  ...   \n",
       "\n",
       "      (inferno, JJ)  (lol, VBP)  (american-canadian, JJ)  (hardgor, JJ)  \\\n",
       "0                 0           0                        0              0   \n",
       "1                 0           0                        0              0   \n",
       "2                 0           0                        0              0   \n",
       "3                 0           0                        0              0   \n",
       "4                 0           0                        0              0   \n",
       "...             ...         ...                      ...            ...   \n",
       "4995              0           0                        0              0   \n",
       "4996              0           0                        0              0   \n",
       "4997              1           1                        1              0   \n",
       "4998              0           0                        0              1   \n",
       "4999              0           0                        0              0   \n",
       "\n",
       "      (chronicl, VBP)  (lego, NN)  (whelk, JJ)  (18,000, CD)  (ill, NNS)  \\\n",
       "0                   0           0            0             0           0   \n",
       "1                   0           0            0             0           0   \n",
       "2                   0           0            0             0           0   \n",
       "3                   0           0            0             0           0   \n",
       "4                   0           0            0             0           0   \n",
       "...               ...         ...          ...           ...         ...   \n",
       "4995                0           0            0             0           0   \n",
       "4996                0           0            0             0           0   \n",
       "4997                0           0            0             0           0   \n",
       "4998                1           1            1             1           0   \n",
       "4999                0           0            0             0           1   \n",
       "\n",
       "      (shite, JJ)  \n",
       "0               0  \n",
       "1               0  \n",
       "2               0  \n",
       "3               0  \n",
       "4               0  \n",
       "...           ...  \n",
       "4995            0  \n",
       "4996            0  \n",
       "4997            0  \n",
       "4998            0  \n",
       "4999            1  \n",
       "\n",
       "[5000 rows x 52230 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "bag_of_words = []\n",
    "for tokens in new_train['tokens']:\n",
    "    bag_of_words.append(Counter(tokens))\n",
    "    \n",
    "df_bows = pd.DataFrame.from_records(bag_of_words)\n",
    "df_bows = df_bows.fillna(0).astype(int)\n",
    "df_bows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fatal-entry",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-11T19:26:56.852153Z",
     "start_time": "2021-04-11T19:26:49.549262Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file</th>\n",
       "      <th>text</th>\n",
       "      <th>is_positive</th>\n",
       "      <th>docs</th>\n",
       "      <th>tokens</th>\n",
       "      <th>tokens_with_positive</th>\n",
       "      <th>predict_positive</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0_9.txt</td>\n",
       "      <td>b'Bromwell High is a cartoon comedy. It ran at...</td>\n",
       "      <td>1</td>\n",
       "      <td>bromwell high cartoon comedy ran time program ...</td>\n",
       "      <td>[(bromwel, RB), (high, JJ), (cartoon, NN), (co...</td>\n",
       "      <td>[(bromwel, RB, 1), (high, JJ, 1), (cartoon, NN...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10000_8.txt</td>\n",
       "      <td>b'Homelessness (or Houselessness as George Car...</td>\n",
       "      <td>1</td>\n",
       "      <td>homelessness houselessness george carlin state...</td>\n",
       "      <td>[(homeless, NN), (houseless, NN), (georg, NN),...</td>\n",
       "      <td>[(homeless, NN, 1), (houseless, NN, 1), (georg...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10001_10.txt</td>\n",
       "      <td>b'Brilliant over-acting by Lesley Ann Warren. ...</td>\n",
       "      <td>1</td>\n",
       "      <td>brilliant acting lesley ann warren best dramat...</td>\n",
       "      <td>[(brilliant, JJ), (over-act, JJ), (lesley, NN)...</td>\n",
       "      <td>[(brilliant, JJ, 1), (over-act, JJ, 1), (lesle...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10002_7.txt</td>\n",
       "      <td>b'This is easily the most underrated film inn ...</td>\n",
       "      <td>1</td>\n",
       "      <td>easily underrated film inn brook cannon sure f...</td>\n",
       "      <td>[(easili, NN), (underr, JJ), (film, NN), (inn,...</td>\n",
       "      <td>[(easili, NN, 1), (underr, JJ, 1), (film, NN, ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10003_8.txt</td>\n",
       "      <td>b'This is not the typical Mel Brooks film. It ...</td>\n",
       "      <td>1</td>\n",
       "      <td>typical mel brook film slapstick movie actuall...</td>\n",
       "      <td>[(typic, NN), (mel, NN), (brook, NN), (film, N...</td>\n",
       "      <td>[(typic, NN, 1), (mel, NN, 1), (brook, NN, 1),...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4995</th>\n",
       "      <td>9998_4.txt</td>\n",
       "      <td>b\"Towards the end of the movie, I felt it was ...</td>\n",
       "      <td>0</td>\n",
       "      <td>end movie felt technical felt like classroom w...</td>\n",
       "      <td>[(end, NN), (movi, NN), (felt, VBD), (technic,...</td>\n",
       "      <td>[(end, NN, 0), (movi, NN, 0), (felt, VBD, 0), ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4996</th>\n",
       "      <td>9999_3.txt</td>\n",
       "      <td>b'This is the kind of movie that my enemies co...</td>\n",
       "      <td>0</td>\n",
       "      <td>kind movie enemy content watch time bloody tru...</td>\n",
       "      <td>[(kind, NN), (movi, NN), (enemi, NN), (content...</td>\n",
       "      <td>[(kind, NN, 0), (movi, NN, 0), (enemi, NN, 0),...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4997</th>\n",
       "      <td>999_3.txt</td>\n",
       "      <td>b\"I saw 'Descent' last night at the Stockholm ...</td>\n",
       "      <td>0</td>\n",
       "      <td>saw descent night stockholm film festival huge...</td>\n",
       "      <td>[(saw, JJ), (descent, NN), (night, NN), (stock...</td>\n",
       "      <td>[(saw, JJ, 0), (descent, NN, 0), (night, NN, 0...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4998</th>\n",
       "      <td>99_1.txt</td>\n",
       "      <td>b\"Some films that you pick up for a pound turn...</td>\n",
       "      <td>0</td>\n",
       "      <td>film pick pound turn good 23rd century film re...</td>\n",
       "      <td>[(film, NN), (pick, NN), (pound, NN), (turn, V...</td>\n",
       "      <td>[(film, NN, 0), (pick, NN, 0), (pound, NN, 0),...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4999</th>\n",
       "      <td>9_1.txt</td>\n",
       "      <td>b\"This is one of the dumbest films, I've ever ...</td>\n",
       "      <td>0</td>\n",
       "      <td>dumbest film seen rip nearly type thriller man...</td>\n",
       "      <td>[(dumbest, NN), (film, NN), (seen, VBN), (rip,...</td>\n",
       "      <td>[(dumbest, NN, 0), (film, NN, 0), (seen, VBN, ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5000 rows Ã— 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              file                                               text  \\\n",
       "0          0_9.txt  b'Bromwell High is a cartoon comedy. It ran at...   \n",
       "1      10000_8.txt  b'Homelessness (or Houselessness as George Car...   \n",
       "2     10001_10.txt  b'Brilliant over-acting by Lesley Ann Warren. ...   \n",
       "3      10002_7.txt  b'This is easily the most underrated film inn ...   \n",
       "4      10003_8.txt  b'This is not the typical Mel Brooks film. It ...   \n",
       "...            ...                                                ...   \n",
       "4995    9998_4.txt  b\"Towards the end of the movie, I felt it was ...   \n",
       "4996    9999_3.txt  b'This is the kind of movie that my enemies co...   \n",
       "4997     999_3.txt  b\"I saw 'Descent' last night at the Stockholm ...   \n",
       "4998      99_1.txt  b\"Some films that you pick up for a pound turn...   \n",
       "4999       9_1.txt  b\"This is one of the dumbest films, I've ever ...   \n",
       "\n",
       "      is_positive                                               docs  \\\n",
       "0               1  bromwell high cartoon comedy ran time program ...   \n",
       "1               1  homelessness houselessness george carlin state...   \n",
       "2               1  brilliant acting lesley ann warren best dramat...   \n",
       "3               1  easily underrated film inn brook cannon sure f...   \n",
       "4               1  typical mel brook film slapstick movie actuall...   \n",
       "...           ...                                                ...   \n",
       "4995            0  end movie felt technical felt like classroom w...   \n",
       "4996            0  kind movie enemy content watch time bloody tru...   \n",
       "4997            0  saw descent night stockholm film festival huge...   \n",
       "4998            0  film pick pound turn good 23rd century film re...   \n",
       "4999            0  dumbest film seen rip nearly type thriller man...   \n",
       "\n",
       "                                                 tokens  \\\n",
       "0     [(bromwel, RB), (high, JJ), (cartoon, NN), (co...   \n",
       "1     [(homeless, NN), (houseless, NN), (georg, NN),...   \n",
       "2     [(brilliant, JJ), (over-act, JJ), (lesley, NN)...   \n",
       "3     [(easili, NN), (underr, JJ), (film, NN), (inn,...   \n",
       "4     [(typic, NN), (mel, NN), (brook, NN), (film, N...   \n",
       "...                                                 ...   \n",
       "4995  [(end, NN), (movi, NN), (felt, VBD), (technic,...   \n",
       "4996  [(kind, NN), (movi, NN), (enemi, NN), (content...   \n",
       "4997  [(saw, JJ), (descent, NN), (night, NN), (stock...   \n",
       "4998  [(film, NN), (pick, NN), (pound, NN), (turn, V...   \n",
       "4999  [(dumbest, NN), (film, NN), (seen, VBN), (rip,...   \n",
       "\n",
       "                                   tokens_with_positive  predict_positive  \n",
       "0     [(bromwel, RB, 1), (high, JJ, 1), (cartoon, NN...                 1  \n",
       "1     [(homeless, NN, 1), (houseless, NN, 1), (georg...                 1  \n",
       "2     [(brilliant, JJ, 1), (over-act, JJ, 1), (lesle...                 1  \n",
       "3     [(easili, NN, 1), (underr, JJ, 1), (film, NN, ...                 1  \n",
       "4     [(typic, NN, 1), (mel, NN, 1), (brook, NN, 1),...                 1  \n",
       "...                                                 ...               ...  \n",
       "4995  [(end, NN, 0), (movi, NN, 0), (felt, VBD, 0), ...                 0  \n",
       "4996  [(kind, NN, 0), (movi, NN, 0), (enemi, NN, 0),...                 0  \n",
       "4997  [(saw, JJ, 0), (descent, NN, 0), (night, NN, 0...                 0  \n",
       "4998  [(film, NN, 0), (pick, NN, 0), (pound, NN, 0),...                 0  \n",
       "4999  [(dumbest, NN, 0), (film, NN, 0), (seen, VBN, ...                 0  \n",
       "\n",
       "[5000 rows x 7 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "nb = MultinomialNB()\n",
    "nb = nb.fit(df_bows, new_train['is_positive'] == 1) # convert our integer to boolean for Naive Bayes classification\n",
    "predict_df = pd.DataFrame(nb.predict_proba(df_bows)).rename(columns={0: 'prob_negative', 1: 'prob_positive'})\n",
    "predict_df['predict_positive'] = (predict_df['prob_positive'] > 0.5).astype(int)\n",
    "new_train = pd.concat([new_train, predict_df['predict_positive']], axis=1)\n",
    "new_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "second-founder",
   "metadata": {},
   "source": [
    "Determine our train accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "lightweight-hazard",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-11T19:27:04.366522Z",
     "start_time": "2021-04-11T19:27:04.325604Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9768"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(new_train['predict_positive'] == new_train['is_positive']).sum() / len(new_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "powerful-lottery",
   "metadata": {},
   "source": [
    "We have 97.7% train accuracy. We can now use our test data to determine our test accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "functioning-expert",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ongoing-maria",
   "metadata": {},
   "source": [
    "## Bag of N-grams based features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "novel-jurisdiction",
   "metadata": {},
   "source": [
    "Get only 3000 records to avoid out of memory issue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "medium-hospital",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-11T20:57:56.175140Z",
     "start_time": "2021-04-11T20:57:56.154195Z"
    }
   },
   "outputs": [],
   "source": [
    "positive_reviews = train_df[:1500]  # take first 1500 reviews which are positive\n",
    "negative_reviews = train_df[23500:]  # last last 1500 reviews which are negative\n",
    "new_train1 = positive_reviews.append(negative_reviews, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "academic-fabric",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-11T20:57:58.328932Z",
     "start_time": "2021-04-11T20:57:58.304962Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file</th>\n",
       "      <th>text</th>\n",
       "      <th>is_positive</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0_9.txt</td>\n",
       "      <td>b'Bromwell High is a cartoon comedy. It ran at...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10000_8.txt</td>\n",
       "      <td>b'Homelessness (or Houselessness as George Car...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10001_10.txt</td>\n",
       "      <td>b'Brilliant over-acting by Lesley Ann Warren. ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10002_7.txt</td>\n",
       "      <td>b'This is easily the most underrated film inn ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10003_8.txt</td>\n",
       "      <td>b'This is not the typical Mel Brooks film. It ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           file                                               text  \\\n",
       "0       0_9.txt  b'Bromwell High is a cartoon comedy. It ran at...   \n",
       "1   10000_8.txt  b'Homelessness (or Houselessness as George Car...   \n",
       "2  10001_10.txt  b'Brilliant over-acting by Lesley Ann Warren. ...   \n",
       "3   10002_7.txt  b'This is easily the most underrated film inn ...   \n",
       "4   10003_8.txt  b'This is not the typical Mel Brooks film. It ...   \n",
       "\n",
       "   is_positive  \n",
       "0            1  \n",
       "1            1  \n",
       "2            1  \n",
       "3            1  \n",
       "4            1  "
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_train1.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "sensitive-austria",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-11T20:58:10.076234Z",
     "start_time": "2021-04-11T20:58:03.873144Z"
    }
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import movie_reviews #  this is the corpus we will use\n",
    "from nltk.stem.snowball import SnowballStemmer #  Snowball stemmer > Porter stemmer\n",
    "from nltk.tokenize.casual import casual_tokenize  # we use casual tokenize because this is colloquial text\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS as sklearn_stop_words  # sklearn stop words is larger than nltk\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "def decontracted(phrase):\n",
    "    # Taken from https://stackoverflow.com/questions/19790188/expanding-english-language-contractions-in-python\n",
    "    \n",
    "    # specific\n",
    "    phrase = re.sub(r\"won\\'t\", \"will not\", phrase)\n",
    "    phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n",
    "\n",
    "    # general\n",
    "    phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n",
    "    phrase = re.sub(r\"\\'s\", \" is\", phrase)\n",
    "    phrase = re.sub(r\"\\'d\", \" would\", phrase)\n",
    "    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n",
    "    phrase = re.sub(r\"\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n",
    "    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n",
    "    phrase = re.sub(r'[^a-zA-Z0-9\\s]', \" \", phrase,re.I)\n",
    "    return phrase\n",
    "\n",
    "\n",
    "def tokenize_phrase(text):\n",
    "    phrase = decontracted(text.replace('\\\\', '').replace(\"b'\", \"\")) # remove backslashes and replace contractions\n",
    "    tokens = casual_tokenize(phrase, reduce_len=True, strip_handles=True)\n",
    "    normalized_tokens = [x.lower() for x in tokens] #  convert to all lowercase\n",
    "    filtered_tokens = [x for x in normalized_tokens if x not in sklearn_stop_words] #  filter stop words\n",
    "    filtered_tokens = [x for x in filtered_tokens if x and x not in '- \\t\\n.\"\\':[...][\\\\]()/[br]<>*~,;!?'] #  filter punctuations\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(t) for t in filtered_tokens] # perform lemmatization\n",
    "#    stemmed_tokens = [stemmer.stem(w) for w in filtered_tokens] # perform stemming\n",
    "#    pos_tokens = nltk.pos_tag(lemmatized_tokens) # perform part-of-speech tagging\n",
    "    doc=' '.join(lemmatized_tokens)\n",
    "\n",
    "    return doc\n",
    "\n",
    "new_train1['docs'] = new_train1['text'].apply(tokenize_phrase)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "historical-breeding",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-11T20:58:15.176404Z",
     "start_time": "2021-04-11T20:58:15.140502Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file</th>\n",
       "      <th>text</th>\n",
       "      <th>is_positive</th>\n",
       "      <th>docs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0_9.txt</td>\n",
       "      <td>b'Bromwell High is a cartoon comedy. It ran at...</td>\n",
       "      <td>1</td>\n",
       "      <td>bromwell high cartoon comedy ran time program ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10000_8.txt</td>\n",
       "      <td>b'Homelessness (or Houselessness as George Car...</td>\n",
       "      <td>1</td>\n",
       "      <td>homelessness houselessness george carlin state...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10001_10.txt</td>\n",
       "      <td>b'Brilliant over-acting by Lesley Ann Warren. ...</td>\n",
       "      <td>1</td>\n",
       "      <td>brilliant acting lesley ann warren best dramat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10002_7.txt</td>\n",
       "      <td>b'This is easily the most underrated film inn ...</td>\n",
       "      <td>1</td>\n",
       "      <td>easily underrated film inn brook cannon sure f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10003_8.txt</td>\n",
       "      <td>b'This is not the typical Mel Brooks film. It ...</td>\n",
       "      <td>1</td>\n",
       "      <td>typical mel brook film slapstick movie actuall...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2995</th>\n",
       "      <td>9998_4.txt</td>\n",
       "      <td>b\"Towards the end of the movie, I felt it was ...</td>\n",
       "      <td>0</td>\n",
       "      <td>end movie felt technical felt like classroom w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2996</th>\n",
       "      <td>9999_3.txt</td>\n",
       "      <td>b'This is the kind of movie that my enemies co...</td>\n",
       "      <td>0</td>\n",
       "      <td>kind movie enemy content watch time bloody tru...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2997</th>\n",
       "      <td>999_3.txt</td>\n",
       "      <td>b\"I saw 'Descent' last night at the Stockholm ...</td>\n",
       "      <td>0</td>\n",
       "      <td>saw descent night stockholm film festival huge...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2998</th>\n",
       "      <td>99_1.txt</td>\n",
       "      <td>b\"Some films that you pick up for a pound turn...</td>\n",
       "      <td>0</td>\n",
       "      <td>film pick pound turn good 23rd century film re...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2999</th>\n",
       "      <td>9_1.txt</td>\n",
       "      <td>b\"This is one of the dumbest films, I've ever ...</td>\n",
       "      <td>0</td>\n",
       "      <td>dumbest film seen rip nearly type thriller man...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3000 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              file                                               text  \\\n",
       "0          0_9.txt  b'Bromwell High is a cartoon comedy. It ran at...   \n",
       "1      10000_8.txt  b'Homelessness (or Houselessness as George Car...   \n",
       "2     10001_10.txt  b'Brilliant over-acting by Lesley Ann Warren. ...   \n",
       "3      10002_7.txt  b'This is easily the most underrated film inn ...   \n",
       "4      10003_8.txt  b'This is not the typical Mel Brooks film. It ...   \n",
       "...            ...                                                ...   \n",
       "2995    9998_4.txt  b\"Towards the end of the movie, I felt it was ...   \n",
       "2996    9999_3.txt  b'This is the kind of movie that my enemies co...   \n",
       "2997     999_3.txt  b\"I saw 'Descent' last night at the Stockholm ...   \n",
       "2998      99_1.txt  b\"Some films that you pick up for a pound turn...   \n",
       "2999       9_1.txt  b\"This is one of the dumbest films, I've ever ...   \n",
       "\n",
       "      is_positive                                               docs  \n",
       "0               1  bromwell high cartoon comedy ran time program ...  \n",
       "1               1  homelessness houselessness george carlin state...  \n",
       "2               1  brilliant acting lesley ann warren best dramat...  \n",
       "3               1  easily underrated film inn brook cannon sure f...  \n",
       "4               1  typical mel brook film slapstick movie actuall...  \n",
       "...           ...                                                ...  \n",
       "2995            0  end movie felt technical felt like classroom w...  \n",
       "2996            0  kind movie enemy content watch time bloody tru...  \n",
       "2997            0  saw descent night stockholm film festival huge...  \n",
       "2998            0  film pick pound turn good 23rd century film re...  \n",
       "2999            0  dumbest film seen rip nearly type thriller man...  \n",
       "\n",
       "[3000 rows x 4 columns]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_train1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lesser-merchant",
   "metadata": {},
   "source": [
    "Select only 600 records to get Bag of 2 Grams feature to avoid out of memory issue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "nearby-behalf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-11T20:58:21.319725Z",
     "start_time": "2021-04-11T20:58:21.305762Z"
    }
   },
   "outputs": [],
   "source": [
    "new_train1_sub=pd.concat([new_train1.head(300), new_train1.tail(300)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "contrary-minority",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-11T20:58:27.572475Z",
     "start_time": "2021-04-11T20:58:27.552527Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file</th>\n",
       "      <th>text</th>\n",
       "      <th>is_positive</th>\n",
       "      <th>docs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0_9.txt</td>\n",
       "      <td>b'Bromwell High is a cartoon comedy. It ran at...</td>\n",
       "      <td>1</td>\n",
       "      <td>bromwell high cartoon comedy ran time program ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10000_8.txt</td>\n",
       "      <td>b'Homelessness (or Houselessness as George Car...</td>\n",
       "      <td>1</td>\n",
       "      <td>homelessness houselessness george carlin state...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10001_10.txt</td>\n",
       "      <td>b'Brilliant over-acting by Lesley Ann Warren. ...</td>\n",
       "      <td>1</td>\n",
       "      <td>brilliant acting lesley ann warren best dramat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10002_7.txt</td>\n",
       "      <td>b'This is easily the most underrated film inn ...</td>\n",
       "      <td>1</td>\n",
       "      <td>easily underrated film inn brook cannon sure f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10003_8.txt</td>\n",
       "      <td>b'This is not the typical Mel Brooks film. It ...</td>\n",
       "      <td>1</td>\n",
       "      <td>typical mel brook film slapstick movie actuall...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2995</th>\n",
       "      <td>9998_4.txt</td>\n",
       "      <td>b\"Towards the end of the movie, I felt it was ...</td>\n",
       "      <td>0</td>\n",
       "      <td>end movie felt technical felt like classroom w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2996</th>\n",
       "      <td>9999_3.txt</td>\n",
       "      <td>b'This is the kind of movie that my enemies co...</td>\n",
       "      <td>0</td>\n",
       "      <td>kind movie enemy content watch time bloody tru...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2997</th>\n",
       "      <td>999_3.txt</td>\n",
       "      <td>b\"I saw 'Descent' last night at the Stockholm ...</td>\n",
       "      <td>0</td>\n",
       "      <td>saw descent night stockholm film festival huge...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2998</th>\n",
       "      <td>99_1.txt</td>\n",
       "      <td>b\"Some films that you pick up for a pound turn...</td>\n",
       "      <td>0</td>\n",
       "      <td>film pick pound turn good 23rd century film re...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2999</th>\n",
       "      <td>9_1.txt</td>\n",
       "      <td>b\"This is one of the dumbest films, I've ever ...</td>\n",
       "      <td>0</td>\n",
       "      <td>dumbest film seen rip nearly type thriller man...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>600 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              file                                               text  \\\n",
       "0          0_9.txt  b'Bromwell High is a cartoon comedy. It ran at...   \n",
       "1      10000_8.txt  b'Homelessness (or Houselessness as George Car...   \n",
       "2     10001_10.txt  b'Brilliant over-acting by Lesley Ann Warren. ...   \n",
       "3      10002_7.txt  b'This is easily the most underrated film inn ...   \n",
       "4      10003_8.txt  b'This is not the typical Mel Brooks film. It ...   \n",
       "...            ...                                                ...   \n",
       "2995    9998_4.txt  b\"Towards the end of the movie, I felt it was ...   \n",
       "2996    9999_3.txt  b'This is the kind of movie that my enemies co...   \n",
       "2997     999_3.txt  b\"I saw 'Descent' last night at the Stockholm ...   \n",
       "2998      99_1.txt  b\"Some films that you pick up for a pound turn...   \n",
       "2999       9_1.txt  b\"This is one of the dumbest films, I've ever ...   \n",
       "\n",
       "      is_positive                                               docs  \n",
       "0               1  bromwell high cartoon comedy ran time program ...  \n",
       "1               1  homelessness houselessness george carlin state...  \n",
       "2               1  brilliant acting lesley ann warren best dramat...  \n",
       "3               1  easily underrated film inn brook cannon sure f...  \n",
       "4               1  typical mel brook film slapstick movie actuall...  \n",
       "...           ...                                                ...  \n",
       "2995            0  end movie felt technical felt like classroom w...  \n",
       "2996            0  kind movie enemy content watch time bloody tru...  \n",
       "2997            0  saw descent night stockholm film festival huge...  \n",
       "2998            0  film pick pound turn good 23rd century film re...  \n",
       "2999            0  dumbest film seen rip nearly type thriller man...  \n",
       "\n",
       "[600 rows x 4 columns]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_train1_sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "rotary-freeze",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-11T20:59:29.805631Z",
     "start_time": "2021-04-11T20:59:29.569077Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "bv = CountVectorizer(ngram_range=(2, 2))\n",
    "bv_matrix =bv.fit_transform(new_train1_sub['docs'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "military-fishing",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-11T20:59:31.542674Z",
     "start_time": "2021-04-11T20:59:31.377597Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>00 abc</th>\n",
       "      <th>000 budget</th>\n",
       "      <th>000 continuity</th>\n",
       "      <th>000 plus</th>\n",
       "      <th>000 produce</th>\n",
       "      <th>007 scene</th>\n",
       "      <th>02 lifetime</th>\n",
       "      <th>05 wild</th>\n",
       "      <th>06 patrick</th>\n",
       "      <th>10 000</th>\n",
       "      <th>...</th>\n",
       "      <th>zoom cut</th>\n",
       "      <th>zoom face</th>\n",
       "      <th>zords obviously</th>\n",
       "      <th>zorro ashamed</th>\n",
       "      <th>zu warrior</th>\n",
       "      <th>zubeidaa fiza</th>\n",
       "      <th>zubeidaa nana</th>\n",
       "      <th>zyuranger series</th>\n",
       "      <th>zzzzzz oh</th>\n",
       "      <th>zzzzzz ooops</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>595</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>596</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>597</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>598</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>599</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>600 rows Ã— 55201 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     00 abc  000 budget  000 continuity  000 plus  000 produce  007 scene  \\\n",
       "0         0           0               0         0            0          0   \n",
       "1         0           0               0         0            0          0   \n",
       "2         0           0               0         0            0          0   \n",
       "3         0           0               0         0            0          0   \n",
       "4         0           0               0         0            0          0   \n",
       "..      ...         ...             ...       ...          ...        ...   \n",
       "595       0           0               0         0            0          0   \n",
       "596       0           0               0         0            0          0   \n",
       "597       0           0               0         0            0          0   \n",
       "598       0           0               1         0            0          0   \n",
       "599       0           0               0         0            0          0   \n",
       "\n",
       "     02 lifetime  05 wild  06 patrick  10 000  ...  zoom cut  zoom face  \\\n",
       "0              0        0           0       0  ...         0          0   \n",
       "1              0        0           0       0  ...         0          0   \n",
       "2              0        0           0       0  ...         0          0   \n",
       "3              0        0           0       0  ...         0          0   \n",
       "4              0        0           0       0  ...         0          0   \n",
       "..           ...      ...         ...     ...  ...       ...        ...   \n",
       "595            0        0           0       0  ...         0          0   \n",
       "596            0        0           0       0  ...         0          0   \n",
       "597            0        0           0       0  ...         0          0   \n",
       "598            0        0           0       0  ...         0          0   \n",
       "599            0        0           0       0  ...         0          0   \n",
       "\n",
       "     zords obviously  zorro ashamed  zu warrior  zubeidaa fiza  zubeidaa nana  \\\n",
       "0                  0              0           0              0              0   \n",
       "1                  0              0           0              0              0   \n",
       "2                  0              0           0              0              0   \n",
       "3                  0              0           0              0              0   \n",
       "4                  0              0           0              0              0   \n",
       "..               ...            ...         ...            ...            ...   \n",
       "595                0              0           0              0              0   \n",
       "596                0              0           0              0              0   \n",
       "597                0              0           0              0              0   \n",
       "598                0              0           0              0              0   \n",
       "599                0              0           0              0              0   \n",
       "\n",
       "     zyuranger series  zzzzzz oh  zzzzzz ooops  \n",
       "0                   0          0             0  \n",
       "1                   0          0             0  \n",
       "2                   0          0             0  \n",
       "3                   0          0             0  \n",
       "4                   0          0             0  \n",
       "..                ...        ...           ...  \n",
       "595                 0          0             0  \n",
       "596                 0          0             0  \n",
       "597                 0          0             0  \n",
       "598                 0          0             0  \n",
       "599                 0          0             0  \n",
       "\n",
       "[600 rows x 55201 columns]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bv_matrix = bv_matrix.toarray()\n",
    "vocab = bv.get_feature_names()\n",
    "df_ngram=pd.DataFrame(bv_matrix, columns=vocab)\n",
    "df_ngram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "equivalent-template",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-11T20:59:35.569436Z",
     "start_time": "2021-04-11T20:59:34.586573Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prob_negative</th>\n",
       "      <th>prob_positive</th>\n",
       "      <th>predict_positive</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.053298e-20</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.272864e-52</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.730365e-21</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.153929e-16</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.118952e-14</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>595</th>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.315143e-38</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>596</th>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.116911e-20</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>597</th>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>4.201518e-43</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>598</th>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>3.890051e-32</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>599</th>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.352085e-19</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>600 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     prob_negative  prob_positive  predict_positive\n",
       "0     5.053298e-20   1.000000e+00                 1\n",
       "1     1.272864e-52   1.000000e+00                 1\n",
       "2     2.730365e-21   1.000000e+00                 1\n",
       "3     1.153929e-16   1.000000e+00                 1\n",
       "4     4.118952e-14   1.000000e+00                 1\n",
       "..             ...            ...               ...\n",
       "595   1.000000e+00   1.315143e-38                 0\n",
       "596   1.000000e+00   1.116911e-20                 0\n",
       "597   1.000000e+00   4.201518e-43                 0\n",
       "598   1.000000e+00   3.890051e-32                 0\n",
       "599   1.000000e+00   1.352085e-19                 0\n",
       "\n",
       "[600 rows x 3 columns]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "nb = MultinomialNB()\n",
    "nb = nb.fit(df_ngram, new_train1_sub['is_positive'] == 1.0) # convert our integer to boolean for Naive Bayes classification\n",
    "predict_df_ngram = pd.DataFrame(nb.predict_proba(df_ngram)).rename(columns={0: 'prob_negative', 1: 'prob_positive'})\n",
    "predict_df_ngram['predict_positive'] = (predict_df_ngram['prob_positive'] > 0.5).astype(int)\n",
    "predict_df_ngram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "stainless-springer",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-11T20:59:38.569544Z",
     "start_time": "2021-04-11T20:59:38.548601Z"
    }
   },
   "outputs": [],
   "source": [
    "new_train1_sub_ngram=new_train1_sub.reset_index()\n",
    "new_train1_sub_ngram\n",
    "new_train1_sub_ngram = pd.concat([new_train1_sub_ngram, predict_df_ngram['predict_positive']],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "corporate-mattress",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-11T20:59:40.981507Z",
     "start_time": "2021-04-11T20:59:40.952554Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>file</th>\n",
       "      <th>text</th>\n",
       "      <th>is_positive</th>\n",
       "      <th>docs</th>\n",
       "      <th>predict_positive</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0_9.txt</td>\n",
       "      <td>b'Bromwell High is a cartoon comedy. It ran at...</td>\n",
       "      <td>1</td>\n",
       "      <td>bromwell high cartoon comedy ran time program ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>10000_8.txt</td>\n",
       "      <td>b'Homelessness (or Houselessness as George Car...</td>\n",
       "      <td>1</td>\n",
       "      <td>homelessness houselessness george carlin state...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>10001_10.txt</td>\n",
       "      <td>b'Brilliant over-acting by Lesley Ann Warren. ...</td>\n",
       "      <td>1</td>\n",
       "      <td>brilliant acting lesley ann warren best dramat...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>10002_7.txt</td>\n",
       "      <td>b'This is easily the most underrated film inn ...</td>\n",
       "      <td>1</td>\n",
       "      <td>easily underrated film inn brook cannon sure f...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>10003_8.txt</td>\n",
       "      <td>b'This is not the typical Mel Brooks film. It ...</td>\n",
       "      <td>1</td>\n",
       "      <td>typical mel brook film slapstick movie actuall...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>595</th>\n",
       "      <td>2995</td>\n",
       "      <td>9998_4.txt</td>\n",
       "      <td>b\"Towards the end of the movie, I felt it was ...</td>\n",
       "      <td>0</td>\n",
       "      <td>end movie felt technical felt like classroom w...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>596</th>\n",
       "      <td>2996</td>\n",
       "      <td>9999_3.txt</td>\n",
       "      <td>b'This is the kind of movie that my enemies co...</td>\n",
       "      <td>0</td>\n",
       "      <td>kind movie enemy content watch time bloody tru...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>597</th>\n",
       "      <td>2997</td>\n",
       "      <td>999_3.txt</td>\n",
       "      <td>b\"I saw 'Descent' last night at the Stockholm ...</td>\n",
       "      <td>0</td>\n",
       "      <td>saw descent night stockholm film festival huge...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>598</th>\n",
       "      <td>2998</td>\n",
       "      <td>99_1.txt</td>\n",
       "      <td>b\"Some films that you pick up for a pound turn...</td>\n",
       "      <td>0</td>\n",
       "      <td>film pick pound turn good 23rd century film re...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>599</th>\n",
       "      <td>2999</td>\n",
       "      <td>9_1.txt</td>\n",
       "      <td>b\"This is one of the dumbest films, I've ever ...</td>\n",
       "      <td>0</td>\n",
       "      <td>dumbest film seen rip nearly type thriller man...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>600 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     index          file                                               text  \\\n",
       "0        0       0_9.txt  b'Bromwell High is a cartoon comedy. It ran at...   \n",
       "1        1   10000_8.txt  b'Homelessness (or Houselessness as George Car...   \n",
       "2        2  10001_10.txt  b'Brilliant over-acting by Lesley Ann Warren. ...   \n",
       "3        3   10002_7.txt  b'This is easily the most underrated film inn ...   \n",
       "4        4   10003_8.txt  b'This is not the typical Mel Brooks film. It ...   \n",
       "..     ...           ...                                                ...   \n",
       "595   2995    9998_4.txt  b\"Towards the end of the movie, I felt it was ...   \n",
       "596   2996    9999_3.txt  b'This is the kind of movie that my enemies co...   \n",
       "597   2997     999_3.txt  b\"I saw 'Descent' last night at the Stockholm ...   \n",
       "598   2998      99_1.txt  b\"Some films that you pick up for a pound turn...   \n",
       "599   2999       9_1.txt  b\"This is one of the dumbest films, I've ever ...   \n",
       "\n",
       "     is_positive                                               docs  \\\n",
       "0              1  bromwell high cartoon comedy ran time program ...   \n",
       "1              1  homelessness houselessness george carlin state...   \n",
       "2              1  brilliant acting lesley ann warren best dramat...   \n",
       "3              1  easily underrated film inn brook cannon sure f...   \n",
       "4              1  typical mel brook film slapstick movie actuall...   \n",
       "..           ...                                                ...   \n",
       "595            0  end movie felt technical felt like classroom w...   \n",
       "596            0  kind movie enemy content watch time bloody tru...   \n",
       "597            0  saw descent night stockholm film festival huge...   \n",
       "598            0  film pick pound turn good 23rd century film re...   \n",
       "599            0  dumbest film seen rip nearly type thriller man...   \n",
       "\n",
       "     predict_positive  \n",
       "0                   1  \n",
       "1                   1  \n",
       "2                   1  \n",
       "3                   1  \n",
       "4                   1  \n",
       "..                ...  \n",
       "595                 0  \n",
       "596                 0  \n",
       "597                 0  \n",
       "598                 0  \n",
       "599                 0  \n",
       "\n",
       "[600 rows x 6 columns]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_train1_sub_ngram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "associate-shoulder",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-11T20:59:47.740586Z",
     "start_time": "2021-04-11T20:59:47.728620Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(new_train1_sub_ngram['predict_positive'] == new_train1_sub_ngram['is_positive']).sum() / len(new_train1_sub_ngram)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "linear-boring",
   "metadata": {},
   "source": [
    "Naive Bayes model based on Bag of 2 Grams features has 100% training accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "parental-airfare",
   "metadata": {},
   "source": [
    "## TF-IDF model based featurs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "crazy-salvation",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-11T20:55:38.699833Z",
     "start_time": "2021-04-11T20:55:38.689828Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tv = TfidfVectorizer(min_df=0., max_df=1., use_idf=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "aging-prefix",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-11T20:58:43.240783Z",
     "start_time": "2021-04-11T20:58:41.798090Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>00</th>\n",
       "      <th>000</th>\n",
       "      <th>0001</th>\n",
       "      <th>007</th>\n",
       "      <th>00s</th>\n",
       "      <th>01</th>\n",
       "      <th>02</th>\n",
       "      <th>029</th>\n",
       "      <th>041</th>\n",
       "      <th>05</th>\n",
       "      <th>...</th>\n",
       "      <th>zorro</th>\n",
       "      <th>zu</th>\n",
       "      <th>zubeidaa</th>\n",
       "      <th>zucker</th>\n",
       "      <th>zuckerman</th>\n",
       "      <th>zuckers</th>\n",
       "      <th>zuzz</th>\n",
       "      <th>zyada</th>\n",
       "      <th>zyuranger</th>\n",
       "      <th>zzzzzz</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 25696 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    00  000  0001  007  00s   01   02  029  041   05  ...  zorro   zu  \\\n",
       "0  0.0  0.0   0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...    0.0  0.0   \n",
       "1  0.0  0.0   0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...    0.0  0.0   \n",
       "2  0.0  0.0   0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...    0.0  0.0   \n",
       "3  0.0  0.0   0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...    0.0  0.0   \n",
       "4  0.0  0.0   0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...    0.0  0.0   \n",
       "\n",
       "   zubeidaa  zucker  zuckerman  zuckers  zuzz  zyada  zyuranger  zzzzzz  \n",
       "0       0.0     0.0        0.0      0.0   0.0    0.0        0.0     0.0  \n",
       "1       0.0     0.0        0.0      0.0   0.0    0.0        0.0     0.0  \n",
       "2       0.0     0.0        0.0      0.0   0.0    0.0        0.0     0.0  \n",
       "3       0.0     0.0        0.0      0.0   0.0    0.0        0.0     0.0  \n",
       "4       0.0     0.0        0.0      0.0   0.0    0.0        0.0     0.0  \n",
       "\n",
       "[5 rows x 25696 columns]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tv_matrix = tv.fit_transform(new_train1['docs'])\n",
    "tv_matrix = tv_matrix.toarray()\n",
    "vocab = tv.get_feature_names()\n",
    "df_tfidf=pd.DataFrame(np.round(tv_matrix, 2), columns=vocab)\n",
    "df_tfidf.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "figured-cooling",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-11T20:58:50.951403Z",
     "start_time": "2021-04-11T20:58:50.281560Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "nb = MultinomialNB()\n",
    "nb_tfidf = nb.fit(df_tfidf, new_train1['is_positive'] == 1) # convert our integer to boolean for Naive Bayes classification\n",
    "predict_df_tfidf = pd.DataFrame(nb_tfidf.predict_proba(df_tfidf)).rename(columns={0: 'prob_negative', 1: 'prob_positive'})\n",
    "predict_df_tfidf['predict_positive'] = (predict_df_tfidf['prob_positive'] > 0.5).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "korean-render",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-11T20:58:53.457519Z",
     "start_time": "2021-04-11T20:58:53.440564Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file</th>\n",
       "      <th>text</th>\n",
       "      <th>is_positive</th>\n",
       "      <th>docs</th>\n",
       "      <th>predict_positive</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0_9.txt</td>\n",
       "      <td>b'Bromwell High is a cartoon comedy. It ran at...</td>\n",
       "      <td>1</td>\n",
       "      <td>bromwell high cartoon comedy ran time program ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10000_8.txt</td>\n",
       "      <td>b'Homelessness (or Houselessness as George Car...</td>\n",
       "      <td>1</td>\n",
       "      <td>homelessness houselessness george carlin state...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10001_10.txt</td>\n",
       "      <td>b'Brilliant over-acting by Lesley Ann Warren. ...</td>\n",
       "      <td>1</td>\n",
       "      <td>brilliant acting lesley ann warren best dramat...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10002_7.txt</td>\n",
       "      <td>b'This is easily the most underrated film inn ...</td>\n",
       "      <td>1</td>\n",
       "      <td>easily underrated film inn brook cannon sure f...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10003_8.txt</td>\n",
       "      <td>b'This is not the typical Mel Brooks film. It ...</td>\n",
       "      <td>1</td>\n",
       "      <td>typical mel brook film slapstick movie actuall...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           file                                               text  \\\n",
       "0       0_9.txt  b'Bromwell High is a cartoon comedy. It ran at...   \n",
       "1   10000_8.txt  b'Homelessness (or Houselessness as George Car...   \n",
       "2  10001_10.txt  b'Brilliant over-acting by Lesley Ann Warren. ...   \n",
       "3   10002_7.txt  b'This is easily the most underrated film inn ...   \n",
       "4   10003_8.txt  b'This is not the typical Mel Brooks film. It ...   \n",
       "\n",
       "   is_positive                                               docs  \\\n",
       "0            1  bromwell high cartoon comedy ran time program ...   \n",
       "1            1  homelessness houselessness george carlin state...   \n",
       "2            1  brilliant acting lesley ann warren best dramat...   \n",
       "3            1  easily underrated film inn brook cannon sure f...   \n",
       "4            1  typical mel brook film slapstick movie actuall...   \n",
       "\n",
       "   predict_positive  \n",
       "0                 1  \n",
       "1                 1  \n",
       "2                 1  \n",
       "3                 1  \n",
       "4                 1  "
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_train_tfidf=new_train1\n",
    "new_train_tfidf = pd.concat([new_train_tfidf, predict_df_tfidf['predict_positive']], axis=1)\n",
    "new_train_tfidf.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "crucial-violin",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-11T20:59:05.322510Z",
     "start_time": "2021-04-11T20:59:05.305554Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9793333333333333"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(new_train_tfidf['predict_positive'] == new_train_tfidf['is_positive']).sum() / len(new_train_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "southeast-doubt",
   "metadata": {},
   "source": [
    "Naive Bayes model based on TF-IDF features has 97.9% training accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "existing-blink",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "northern-dress",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "passing-documentary",
   "metadata": {},
   "outputs": [],
   "source": [
    "# following tutorial from https://www.guru99.com/stemming-lemmatization-python-nltk.html\n",
    "\n",
    "# Wordnet Lemmatizer (with Part of Speech tag)\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "nltk.download(\"averaged_perceptron_tagger\")\n",
    "nltk.download(\"wordnet\")\n",
    "from nltk.corpus import wordnet as wn  # import corpus reader wordnet\n",
    "from nltk import word_tokenize, pos_tag  # import word_tokenize and parts of speech tag\n",
    "from collections import defaultdict\n",
    "\n",
    "# part of speech tag\n",
    "tag_map = defaultdict(lambda: wn.NOUN)  # noun\n",
    "tag_map[\"J\"] = wn.ADJ  # adjective\n",
    "tag_map[\"V\"] = wn.VERB  # verb\n",
    "tag_map[\"R\"] = wn.ADV  # adverb\n",
    "\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "\n",
    "def tokenize_phrase_no_stemming(text):\n",
    "    phrase = decontracted(\n",
    "        text.replace(\"\\\\\", \"\").replace(\"b'\", \"\")\n",
    "    )  # remove backslashes and replace contractions\n",
    "    tokens = casual_tokenize(phrase, reduce_len=True, strip_handles=True)\n",
    "    normalized_tokens = [x.lower() for x in tokens]  #  convert to all lowercase\n",
    "    filtered_tokens = [\n",
    "        x for x in normalized_tokens if x not in sklearn_stop_words\n",
    "    ]  #  filter stop words\n",
    "    filtered_tokens = [\n",
    "        x\n",
    "        for x in filtered_tokens\n",
    "        if x and x not in \"- \\t\\n.\\\"':[...][\\\\]()/[br]<>*~,;!?\"\n",
    "    ]  #  filter punctuations\n",
    "    return tokens\n",
    "\n",
    "\n",
    "tokens = tokenize_phrase(str(new_train[\"text\"][2]))\n",
    "\n",
    "for token, tag in pos_tag(tokens):\n",
    "    lemma = wordnet_lemmatizer.lemmatize(token, tag_map[tag[0]])\n",
    "    print(token, \"=>\", lemma)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "256px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
